{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQAcVVMKOIV9",
        "outputId": "bc6ccd40-b070-4410-c724-be7b44c69f13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-5DHIV5J1vE",
        "outputId": "879db10d-97bf-4fe8-9a38-60cdd6273f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [0.9999999999999992]\n",
            "Intercept: 15.000000000000009\n"
          ]
        }
      ],
      "source": [
        "# Example: Linear Regression with Spark MLlib --> clustering regresi\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
        "\n",
        "# Load sample data\n",
        "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
        "columns = ['ID', 'Feature', 'Target']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Prepare data for modeling\n",
        "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
        "df_transformed = assembler.transform(df)\n",
        "\n",
        "# Train a linear regression model\n",
        "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
        "model = lr.fit(df_transformed)\n",
        "\n",
        "# Print model coefficients\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice: Logistic Regression\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Inisialisasi sesi Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Logistic Regression\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, 2.0, 3.0, 0), (2, 1.0, 5.0, 1), (3, 2.5, 4.5, 1), (4, 3.0, 6.0, 0)]\n",
        "columns = ['ID', 'x1', 'x2', 'Label']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['x1', 'x2'], outputCol='Features')\n",
        "df_features = assembler.transform(df)\n",
        "\n",
        "# Train logistic regression model\n",
        "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
        "model = lr.fit(df_features)\n",
        "\n",
        "# Display coefficients and summary\n",
        "print(f'Coefficients: {model.coefficients}')\n",
        "print(f'Intercept: {model.intercept}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AquEvw4mw573",
        "outputId": "0828bea9-aa42-410b-a300-5e9db64e9213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficients: [-12.262057929180484,4.087352266486688]\n",
            "Intercept: 11.56891272665312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Practice: KMeans Clustering\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"KMeans Clustering\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Example dataset\n",
        "data = [(1, 1.0, 1.0), (2, 5.0, 5.0), (3, 10.0, 10.0), (4, 15.0, 15.0)]\n",
        "columns = ['ID', 'x', 'y']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "assembler = VectorAssembler(inputCols=['x', 'y'], outputCol='Features')\n",
        "df_features = assembler.transform(df)\n",
        "\n",
        "# Train KMeans clustering model\n",
        "kmeans = KMeans(featuresCol='Features', k=2)\n",
        "model = kmeans.fit(df_features)\n",
        "\n",
        "# Show cluster centers\n",
        "centers = model.clusterCenters()\n",
        "print(f'Cluster Centers: {centers}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpmFx1EO4p91",
        "outputId": "a5fa4b3e-4057-4bae-80b0-6e08c00ec65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TUGASSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS"
      ],
      "metadata": {
        "id": "M_hL8Ds8ODKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Membuat sesi Spark\n",
        "spark = SparkSession.builder.appName(\"Analisis Dataset\").getOrCreate()\n",
        "\n",
        "# Memuat dataset\n",
        "data = spark.read.csv(\"/content/finance_dataset.csv\", header=True, inferSchema=True)\n",
        "data.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fmRCs3H74vP",
        "outputId": "b279c677-fbd2-4feb-bdf0-c00de80aa858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+-----------+------------------+----------------+--------------+---------------+-----------+-----------+------------------+\n",
            "| ID|               Date|Customer_ID|Transaction_Amount|Transaction_Type|Payment_Method|Account_Balance|   Category|   Location|Transaction_Status|\n",
            "+---+-------------------+-----------+------------------+----------------+--------------+---------------+-----------+-----------+------------------+\n",
            "|  1|2023-01-01 00:00:00|    CUST001|          27337.49|        Transfer| Bank Transfer|      804821.06|Electronics|   New York|           Pending|\n",
            "|  2|2023-01-02 00:00:00|    CUST002|           97716.6|        Purchase|   Credit Card|      540169.76|Electronics|    Houston|           Pending|\n",
            "|  3|2023-01-03 00:00:00|    CUST003|           5752.36|        Transfer|    Debit Card|       925251.6|Electronics|   New York|         Completed|\n",
            "|  4|2023-01-04 00:00:00|    CUST004|          93443.22|        Transfer|    Debit Card|      704136.27|    Savings|   New York|            Failed|\n",
            "|  5|2023-01-05 00:00:00|    CUST005|          15109.98|        Purchase| Bank Transfer|      285987.99|Electronics|    Houston|           Pending|\n",
            "|  6|2023-01-06 00:00:00|    CUST006|          59820.18|        Transfer| Bank Transfer|      392812.96|  Groceries|      Miami|            Failed|\n",
            "|  7|2023-01-07 00:00:00|    CUST007|          30221.04|        Transfer|   Credit Card|      196268.97|Electronics|    Houston|            Failed|\n",
            "|  8|2023-01-08 00:00:00|    CUST008|            4602.7|        Transfer|    Debit Card|      231101.98|  Groceries|      Miami|            Failed|\n",
            "|  9|2023-01-09 00:00:00|    CUST009|          28959.12|        Transfer|    Debit Card|      467473.05|   Clothing|Los Angeles|         Completed|\n",
            "| 10|2023-01-10 00:00:00|    CUST010|          72098.18|        Transfer|   Credit Card|      184918.69|    Savings|    Houston|         Completed|\n",
            "+---+-------------------+-----------+------------------+----------------+--------------+---------------+-----------+-----------+------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "\n",
        "# Menghapus baris dengan nilai yang hilang\n",
        "data = data.na.drop()\n",
        "\n",
        "# Mengubah kolom kategorikal menjadi numerik\n",
        "indexer = StringIndexer(inputCol=\"Transaction_Status\", outputCol=\"label\").fit(data)\n",
        "data = indexer.transform(data)\n",
        "\n",
        "# Menggabungkan fitur menjadi satu vektor\n",
        "assembler = VectorAssembler(inputCols=[\"Transaction_Amount\", \"Account_Balance\"], outputCol=\"features\")\n",
        "final_data = assembler.transform(data).select(\"features\", \"label\")\n",
        "\n",
        "# Menampilkan beberapa baris dari data yang sudah siap\n",
        "final_data.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEh1wALbTNC4",
        "outputId": "521c3f94-bda7-4c46-cf86-1ba0d55fcec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|[27337.49,804821.06]|  2.0|\n",
            "| [97716.6,540169.76]|  2.0|\n",
            "|  [5752.36,925251.6]|  0.0|\n",
            "|[93443.22,704136.27]|  1.0|\n",
            "|[15109.98,285987.99]|  2.0|\n",
            "|[59820.18,392812.96]|  1.0|\n",
            "|[30221.04,196268.97]|  1.0|\n",
            "|  [4602.7,231101.98]|  1.0|\n",
            "|[28959.12,467473.05]|  0.0|\n",
            "|[72098.18,184918.69]|  0.0|\n",
            "+--------------------+-----+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Memisahkan data menjadi set pelatihan dan pengujian\n",
        "train, test = final_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Membuat model Regresi Logistik\n",
        "lr = LogisticRegression()\n",
        "model = lr.fit(train)\n",
        "\n",
        "# Membuat prediksi\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Mengevaluasi model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f\"Akurasi: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fUGOTC2TTNH",
        "outputId": "0e051fa2-aab8-47e1-8676-6b06f24562c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.32553558236887464\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "\n",
        "# Membuat grid parameter\n",
        "paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()\n",
        "\n",
        "# Membuat CrossValidator\n",
        "crossval = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=3)\n",
        "cv_model = crossval.fit(train)\n",
        "\n",
        "# Mengevaluasi model terbaik\n",
        "cv_predictions = cv_model.transform(test)\n",
        "cv_accuracy = evaluator.evaluate(cv_predictions)\n",
        "print(f\"Akurasi setelah Cross-Validation: {cv_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPPX-v4YTWBQ",
        "outputId": "b03d69b3-66dd-4d02-ce28-6a0e1c5ce9b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi setelah Cross-Validation: 0.3282442748091603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U8XtKVwkWkaP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}